{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1.导包"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 文本窗口大小\n",
    "C = 5\n",
    "\n",
    "# 负样本的数目\n",
    "K = 15\n",
    "\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.读取文本数据并处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "with open('./data/text8.train.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 分割文本为单词列表\n",
    "text = text.lower().split()\n",
    "# 得到单词字典表，key是单词、value是出现的次数\n",
    "vocab_dict = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
    "# 将不常用单词都编码为\"<UNK>\"\n",
    "vocab_dict['<UNK>'] = len(text) - np.sum(list(vocab_dict.values()))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab_dict.keys())}\n",
    "idx2word = {idx: word for idx, word in enumerate(vocab_dict.keys())}\n",
    "\n",
    "word_counts = np.array([count for count in vocab_dict.values()], dtype=np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "# 论文里将单词的频率变为原来的0.75次方\n",
    "word_freqs = word_freqs ** (3. / 4.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 构建数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(4813),\n tensor([  10,  419,   50, 9999,  393, 3139,   11,    5,  194,    1]),\n tensor([ 338, 1930,  608, 1466,   34,  126, 2807, 6407, 3822,   74,   13, 4649,\n         9999,  425, 1385,  285,  236, 6396, 2119, 5071, 1651,   41,  274,  878,\n          220,  317,   18,  177,  626,  703, 2405, 3659, 5951,   33, 5391,  139,\n         1622,   18,   10, 4686,  137,   60,  664,   13, 1483, 3363,  673,   29,\n         9999,   19, 4841, 9999, 4520, 9999, 2588, 6291,  122, 5034,  650,    9,\n         1581, 9999,  427,  614,   77, 5074, 2753,   12,   91, 2980, 5395,  569,\n         2098, 1172,  193,    0, 9603, 3606, 5405, 3304, 5036, 4687,  278, 8863,\n          275, 7728, 1361,   40, 1784, 3220, 6374, 9999, 1791, 9999,  390, 4027,\n         4840,   45, 9935, 1312, 4440, 5552, 1819,  133, 2358,  733, 1047, 3110,\n         1822, 1553, 1211,   54,    0, 4867, 3098,  705, 1100, 6854, 5142, 9999,\n         5419, 4450,  747, 3879, 9470,  254,  334, 6300, 8656,   44, 3390,  284,\n          678, 9999,    6, 6210,  587,   13, 5513,   50, 1854, 6409,  277, 7835,\n            9, 2946, 8799, 3906, 1158,   10]))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, text, word2idx, word_freqs):\n",
    "        \"\"\"\n",
    "        :param text: 单词列表\n",
    "        :param word2idx: 从单词到索引的字典\n",
    "        :param word_freqs: 每个单词出现的频率\n",
    "        \"\"\"\n",
    "        super(SkipGramDataset, self).__init__()\n",
    "        # 将单词数字化表示，若不在字典中，表示为unk的数字化结果\n",
    "        self.text_encoded = [word2idx.get(word, word2idx['<UNK>']) for word in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        self.word2idx = word2idx\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_encoded)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"\n",
    "        :param item: 索引\n",
    "        :return:\n",
    "            - 中心词\n",
    "            - 中心词附近2C个postive word\n",
    "            - 随机采样的K个单词作为negative word\n",
    "        \"\"\"\n",
    "        # 获得中心词\n",
    "        center_word = self.text_encoded[item]\n",
    "        # 先取得中心词左右各C个词的索引\n",
    "        pos_indices = list(range(item - C, item)) + list(range(item + 1, item + C + 1))\n",
    "        # 为了避免索引越界，所以进行取余处理\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # 每采样一个正确的单词(positive word)，就采样K个错误的单词(negative word)\n",
    "        # 取样方式采用有放回的采样，并且self.word_freqs数值越大，取样概率越大\n",
    "        neg_words = torch.multinomial(self.word_freqs, num_samples=K * len(pos_words), replacement=True)\n",
    "        # 保证 neg_words中不能包含positive word\n",
    "        while len(set(pos_indices) & set(neg_words.numpy().tolist())) > 0:\n",
    "            neg_words = torch.multinomial(self.word_freqs, K * len(pos_words), True)\n",
    "\n",
    "        return center_word, pos_words, neg_words\n",
    "\n",
    "\n",
    "dataset = SkipGramDataset(\n",
    "    text=text,\n",
    "    word2idx=word2idx,\n",
    "    word_freqs=word_freqs\n",
    ")\n",
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 构建Skip-Gram模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        \"\"\"\n",
    "        :param input_labels: center words, [batch_size]\n",
    "        :param pos_labels: positive words, [batch_size, (window_size * 2)]\n",
    "        :param neg_labels: negative words, [batch_size, (window_size * 2 * K)]\n",
    "        :return: loss, [batch_size]\n",
    "        \"\"\"\n",
    "        input_embedding = self.in_embed(input_labels)\n",
    "        pos_embedding = self.out_embed(pos_labels)\n",
    "        neg_embedding = self.out_embed(neg_labels)\n",
    "\n",
    "        # [batch_size, embed_size] -> [batch_size, embed_size, 1]\n",
    "        input_embedding = input_embedding.unsqueeze(2)\n",
    "\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding)\n",
    "        pos_dot = pos_dot.squeeze(2)\n",
    "\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding)\n",
    "        neg_dot = neg_dot.squeeze(2)\n",
    "\n",
    "        log_pos = F.logsigmoid(pos_dot).sum(1)\n",
    "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
    "\n",
    "        loss = log_pos + log_neg\n",
    "        return -loss\n",
    "\n",
    "    def input_embedding(self):\n",
    "        return self.in_embed.weight.detach().numpy()\n",
    "\n",
    "model = SkipGramModel(\n",
    "    vocab_size=MAX_VOCAB_SIZE,\n",
    "    embed_size=EMBEDDING_SIZE\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 模型训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, iteration 500, loss: 32.6262092590332\n",
      "Epoch 1, iteration 1000, loss: 32.87359619140625\n",
      "Epoch 1, iteration 1500, loss: 32.77399444580078\n",
      "Epoch 1, iteration 2000, loss: 32.819480895996094\n",
      "Epoch 1, iteration 2500, loss: 32.727874755859375\n",
      "Epoch 1, iteration 3000, loss: 32.81598663330078\n",
      "Epoch 1, iteration 3500, loss: 32.71991729736328\n",
      "Epoch 1, iteration 4000, loss: 32.64341735839844\n",
      "Epoch 1, iteration 4500, loss: 32.81446838378906\n",
      "Epoch 1, iteration 5000, loss: 32.635986328125\n",
      "Epoch 1, iteration 5500, loss: 32.823448181152344\n",
      "Epoch 1, iteration 6000, loss: 32.686309814453125\n",
      "Epoch 1, iteration 6500, loss: 32.77131271362305\n",
      "Epoch 1, iteration 7000, loss: 32.748016357421875\n",
      "Epoch 1, iteration 7500, loss: 33.0582275390625\n",
      "Epoch 1, iteration 8000, loss: 32.71543502807617\n",
      "Epoch 1, iteration 8500, loss: 32.72041320800781\n",
      "Epoch 1, iteration 9000, loss: 32.8436279296875\n",
      "Epoch 1, iteration 9500, loss: 32.858367919921875\n",
      "Epoch 1, iteration 10000, loss: 32.91303253173828\n",
      "Epoch 1, iteration 10500, loss: 32.735679626464844\n",
      "Epoch 1, iteration 11000, loss: 32.945499420166016\n",
      "Epoch 1, iteration 11500, loss: 32.73847579956055\n",
      "Epoch 1, iteration 12000, loss: 32.35858154296875\n",
      "Epoch 1, iteration 12500, loss: 32.74131774902344\n",
      "Epoch 1, iteration 13000, loss: 32.60441970825195\n",
      "Epoch 1, iteration 13500, loss: 32.532508850097656\n",
      "Epoch 1, iteration 14000, loss: 32.753170013427734\n",
      "Epoch 1, iteration 14500, loss: 32.79338073730469\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 1024\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-4)\n",
    "lf = lambda x: ((1 + math.cos(x * math.pi / epochs))/ 2) * (1 - 1e-4) + 1e-4\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for idx, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        input_labels = input_labels.long().to(device)\n",
    "        pos_labels = pos_labels.long().to(device)\n",
    "        neg_labels = neg_labels.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx+1) % 500 == 0:\n",
    "            print('Epoch {}, iteration {}, loss: {}'.format(epoch+1, idx+1, loss.item()))\n",
    "    scheduler.step()\n",
    "\n",
    "torch.save(model.state_dict(), 'embedding-{}.pth'.format(EMBEDDING_SIZE))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6.词向量应用"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = SkipGramModel(\n",
    "    vocab_size=MAX_VOCAB_SIZE,\n",
    "    embed_size=EMBEDDING_SIZE\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('embedding-100.pth'))\n",
    "\n",
    "embedding_weights = model.input_embedding()\n",
    "\n",
    "def find_nearest(word):\n",
    "    \"\"\"找出与某个词相近的一些词\"\"\"\n",
    "    index = word2idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    return [idx2word[i] for i in cos_dis.argsort()[:10]]\n",
    "\n",
    "for word in [\"two\", \"america\", \"computer\"]:\n",
    "    print(word, find_nearest(word))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two ['two', 'three', 'four', 'five', 'six', 'one', 'zero', 'seven', 'eight', 'nine']\n",
      "america ['america', 'europe', 'americas', 'africa', 'caribbean', 'australia', 'atlantic', 'united', 'pacific', 'north']\n",
      "computer ['computer', 'computers', 'hardware', 'graphics', 'video', 'computing', 'software', 'computation', 'console', 'digital']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}